{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the library modules\n",
    "\n",
    "from datetime import datetime\n",
    "import torch\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import *\n",
    "import random\n",
    "from colorama import Fore, Back, Style\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "from seqeval.metrics import precision_score, recall_score, f1_score ,accuracy_score,classification_report\n",
    "from seqeval.scheme import IOB2\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import warnings\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Classes & Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the specific format for saving the read data \n",
    "\n",
    "class TextGetter(object):\n",
    "    \n",
    "    def __init__(self,groupName,data):\n",
    "        self.data = data\n",
    "        self.groupName = groupName\n",
    "        self.empty = False\n",
    "        agg_func = lambda s: [(w, t,Id,Po,Se) for w, t , Id, Po, Se in zip(s[\"Word\"].values.tolist(),\n",
    "                                                     s[\"Tag\"].values.tolist(),\n",
    "                                                     s[\"Id\"].values.tolist(),\n",
    "                                                     s[\"Post #\"].values.tolist(),s[\"Sentence #\"].values.tolist())]\n",
    "        self.grouped = self.data.groupby(self.groupName,sort=False).apply(agg_func)\n",
    "        self.texts = [s for s in self.grouped]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_summary(tag,B_NextI): \n",
    "    pred = ['Sum']*len(tag)\n",
    "    y_actu = pd.Series(tag, name='Label')\n",
    "    y_pred = pd.Series(pred, name='')\n",
    "    df= pd.crosstab(y_actu, y_pred) \n",
    "    df = df.reset_index()\n",
    "            \n",
    "    if B_NextI == True:    \n",
    "        df['Row'] = df['Label'].copy()\n",
    "        for ii in range(len(df['Row'])):\n",
    "            if (df['Row'][ii][:2] == 'B-') or (df['Row'][ii][:2] == 'I-'): \n",
    "                df['Row'][ii] = df['Row'][ii][2:]    \n",
    "        for index in range(len(df)):\n",
    "            if df.loc[index,'Row']=='O':\n",
    "                df.loc[index,'Row']='zzzz'    \n",
    "        \n",
    "        df = df.sort_values(by = ['Row','Label'],axis=0)\n",
    "        df = df.drop('Row',axis= 1)\n",
    "    else:    \n",
    "        for index in range(len(df)):\n",
    "            if df.loc[index,'Label']=='O':\n",
    "                df.loc[index,'Label']='zzzz' \n",
    "                \n",
    "        df = df.sort_values(by = ['Label'],axis=0)\n",
    "        \n",
    "        for index in range(len(df)):\n",
    "            if df.loc[index,'Label']=='zzzz':\n",
    "                df.loc[index,'Label']='O' \n",
    "    \n",
    "    Column_List = df.columns.to_list()\n",
    "    Column_List_New = ['Label']\n",
    "    for lab in TagLabel :\n",
    "        if lab in Column_List:\n",
    "            Column_List_New.extend([lab])\n",
    "    if len(Column_List_New)==1:\n",
    "        Column_List_New = Column_List_New+ ['Sum']\n",
    "       \n",
    "    if B_NextI == True:        \n",
    "        Column_List_New = ['Label']\n",
    "        for lab in TagLabel :\n",
    "            if lab in Column_List:\n",
    "                Column_List_New.extend([lab])\n",
    "        if len(Column_List_New)==1:\n",
    "            Column_List_New = Column_List_New+ ['Sum']\n",
    "        Column_List = Column_List_New\n",
    "          \n",
    "    df = df.reindex( columns=Column_List)          \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for making confusion matrix with and without \"O\" label\n",
    "\n",
    "def Make_Confusion_Graph_Tabel(target,pred,Labels,Prefix,Font_Scale,ShowPercent,Sum_O_Zero): \n",
    "\n",
    "    if ShowPercent:\n",
    "        aa = confusion_matrix(target,pred,labels= Labels,normalize= 'true')\n",
    "        aa = 100.0 * aa\n",
    "        Fmt = '.1f'        \n",
    "    else:\n",
    "        aa = confusion_matrix(target,pred,labels= Labels)\n",
    "        Fmt = 'd'\n",
    "        \n",
    "    if Sum_O_Zero:\n",
    "        aa[len(aa[:,0])-1,len(aa[:,0])-1] = 0\n",
    "        \n",
    "    if Prefix:                          \n",
    "        df_cm = pd.DataFrame(aa, index = [i for i in Labels], columns = [i for i in Labels]) \n",
    "    else:\n",
    "        df_cm = pd.DataFrame(aa, index = [i[2:] if i != 'O' else i for i in Labels], columns = [i[2:] if i != 'O' else i for i in Labels ]) \n",
    "\n",
    "    FigSize =(len(Labels)* 0.8 ,len(Labels)*0.4)\n",
    "    plt.figure(figsize = FigSize)\n",
    "    sn.set(font_scale=Font_Scale) # for label size\n",
    "    sn.heatmap(df_cm, annot=True,cmap='Blues', cbar=False, annot_kws={\"size\": 14},fmt = Fmt)\n",
    "    plt.show()\n",
    "    return df_cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenize the input reviews and organize their proper labels (assign \"O\" to not important tokens)\n",
    "\n",
    "def tokenize_and_preserve_labels(getter_Texts):\n",
    "    Tokenized_Texts =[[]]*len(getter_Texts)\n",
    "    Labels =[[]]*len(getter_Texts)\n",
    "    \n",
    "    for ii,sent in enumerate(getter_Texts):\n",
    "        Tokenized_Texts[ii] =['[CLS]']\n",
    "        Labels[ii] =['O']\n",
    "        token =[]\n",
    "        lab = []\n",
    "        for s in sent:\n",
    "            tokenized_s = tokenizer.tokenize(s[0])\n",
    "            labe_s = s[1]\n",
    "            label =[] \n",
    "            label = [labe_s]*len(tokenized_s)\n",
    "            token.extend(tokenized_s)        \n",
    "            lab.extend(label)    \n",
    "        Tokenized_Texts[ii].extend(token)\n",
    "        Tokenized_Texts[ii].extend(['[SEP]'])\n",
    "        \n",
    "        Labels[ii].extend(lab)\n",
    "        Labels[ii].extend(['O'])\n",
    "    return Tokenized_Texts,Labels           \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert tokenize and their related labels to index\n",
    "\n",
    "def ids_and_mask(text_tokenize,Labels):\n",
    "    text_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in text_tokenize],value=0,\n",
    "                          maxlen=seq_length, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "    \n",
    "    tags = pad_sequences([[tag2idx.get(l) for l in lab] for lab in Labels], maxlen=seq_length,\n",
    "                           value=-100, dtype=\"long\", truncating=\"post\", padding=\"post\") \n",
    "                          \n",
    "    text_mask = [[int(i>0) for i in ii] for ii in text_ids]\n",
    "    \n",
    "    return text_ids,tags,text_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for plotting the loss and accuracy values\n",
    "\n",
    "def plot_loss_accuracy(df_score,xlim_from,xlim_to,yLow_loss,yHigh_loss,yLow_accuracy,flag_title):\n",
    "    sn.set(context=\"notebook\", style=\"white\", palette=None,\n",
    "               font=\"sans-serif\", font_scale=1.1, color_codes=True, rc=None)\n",
    "    df_plot = df_score.copy()\n",
    "    plt.figure(figsize = (16,4))\n",
    "\n",
    "    # Plot negative loss function\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.xlim(xlim_from,xlim_to)\n",
    "    \n",
    "    \n",
    "    df_plot.loc[0,'Tr_loss']=1000\n",
    "    df_plot.loc[0,'Va_loss']=1000\n",
    "\n",
    "    \n",
    "    Tr_loss, = plt.plot(df_plot.Tr_loss)\n",
    "    Va_loss, = plt.plot(df_plot.Va_loss)\n",
    "    \n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('Loos')\n",
    "    plt.yscale('log')\n",
    "    plt.ylim(yLow_loss,yHigh_loss)\n",
    "    plt.grid(ls='--')\n",
    "    if flag_title == 1:\n",
    "        plt.title('Loss - train ends at %.2f | val at %.2f' %(df_plot.tail(1).Tr_loss, df_plot.tail(1).Va_loss))\n",
    "    else:\n",
    "        plt.title('Loss (Training & Validation)')\n",
    "        \n",
    "    val_xmin_loss = np.argmin(np.array(df_plot.Va_loss)) \n",
    "    val_ymin_loss = min(df_plot.Va_loss)\n",
    "    aaa = plt.scatter(val_xmin_loss, val_ymin_loss, marker='o', color='black',lw=2) # plot point    \n",
    "    \n",
    "    plt.legend([Tr_loss, Va_loss,aaa], ['Training', 'Validation','Valid Min'])\n",
    "    \n",
    "    \n",
    "    # Plot accuracy\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.xlim(xlim_from,xlim_to)\n",
    "    Tr_accuracy, = plt.plot(df_plot.Tr_accur)\n",
    "    Va_accuracy, = plt.plot(df_plot.Va_accur)\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('accuracy (%)')\n",
    "    plt.ylim(yLow_accuracy,101)\n",
    "    plt.grid(ls='--')\n",
    "    if flag_title == 1:\n",
    "        plt.title('Accuracy - train ends at %.2f%% | val at %.2f%%' %(df_plot.tail(1).Tr_accur , df_plot.tail(1).Va_accur ))\n",
    "    else:\n",
    "        plt.title('Accuracy (Training & Validation)')\n",
    "\n",
    "    val_xmax_accu = np.argmax(np.array(df_plot.Va_accur))    \n",
    "    val_ymax_accu = max(df_plot.Va_accur)\n",
    "    \n",
    "    aaa = plt.scatter(val_xmax_accu, val_ymax_accu, marker='o', color='black',lw=2) # plot point    \n",
    "    plt.legend([Tr_accuracy, Va_accuracy,aaa], ['Training', 'Validation','Valid Max'])\n",
    "    global fig\n",
    "    fig = plt.gcf()\n",
    "    plt.show(block= False)\n",
    "    \n",
    "    if flag_title == 1:\n",
    "        print(\"Min Valid Loss: %.4f     in Epoch: %.f\" %(val_ymin_loss,val_xmin_loss))\n",
    "        print(\"Max Valid Accu: %.2f %%    in Epoch: %.f\" %(val_ymax_accu,val_xmax_accu)) \n",
    "        print(\"____________________________________________________________\")\n",
    "    return (True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for plotting the F1_score value\n",
    "\n",
    "def plot_F1_Score(df_score,xlim_from,xlim_to,yLow_F1,flag_title): \n",
    "    sn.set(context=\"notebook\", style=\"white\", palette=None,\n",
    "               font=\"sans-serif\", font_scale=1.1, color_codes=True, rc=None)\n",
    "    plt.figure(figsize = (16,4))\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.xlim(xlim_from,xlim_to)\n",
    "    train_F1Score, = plt.plot(df_score.Tr_F1)\n",
    "    valid_F1Score, = plt.plot(df_score.Va_F1)\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('f1-score')\n",
    "    plt.ylim(yLow_F1,1.05)\n",
    "    plt.grid(ls='--')\n",
    "    if flag_title == 1:\n",
    "        plt.title('f1-score - train ends at %.2f | val at %.2f' %(df_score.tail(1).Tr_F1 , df_score.tail(1).Va_F1 ))\n",
    "    else:\n",
    "        plt.title('f1-score (Training & Validation)')\n",
    "        \n",
    "        \n",
    "    valid_xmax_F1 = np.argmax(np.array(df_score.Va_F1))          \n",
    "    valid_ymax_F1 = max(df_score.Va_F1)\n",
    "    aaa = plt.scatter(valid_xmax_F1, valid_ymax_F1, marker='o', color='black',lw=2) # plot point    \n",
    "    plt.legend([train_F1Score, valid_F1Score,aaa], ['Training', 'Validation','Valid Max'])     \n",
    "              \n",
    "    train_ymax_F1 = max(df_score.Tr_F1)\n",
    "    train_xmax_F1 = np.argmax(np.array(df_score.Tr_F1))    \n",
    "    \n",
    "    global fig\n",
    "    fig = plt.gcf()\n",
    "    plt.show(block= False)\n",
    "    if flag_title == 1:\n",
    "        print(\"Training   Max f1-score: %.4f     in Epoch: %.f\" %(train_ymax_F1,train_xmax_F1))\n",
    "        print(\"Validation Max f1-score: %.4f     in Epoch: %.f\" %(valid_ymax_F1,valid_xmax_F1))\n",
    "    print(\"____________________________________________________________\")\n",
    "    return (True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building the model (concatenation BERT 4 last layers + BiLSTM)\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        self.embedding = model_B\n",
    "        \n",
    "        self.linear512_numLabel = nn.Linear(512, num_labels)\n",
    "        self.embedding_dropout = nn.Dropout(0.3)         \n",
    "        self.lstm3072_256 = nn.LSTM(3072, 256,bidirectional=True)\n",
    "        \n",
    "    def forward(self, inputs_data,mask): \n",
    "        with torch.no_grad():\n",
    "            word_emb, sent_emb,hidden_layers = self.embedding(inputs_data,\n",
    "                                                              attention_mask= mask)  \n",
    "            \n",
    "        x = torch.cat([hidden_layers[11], hidden_layers[10],hidden_layers[9], hidden_layers[8]], dim=2)\n",
    "        x = self.embedding_dropout(x)\n",
    "               \n",
    "        x, _ = self.lstm3072_256(x)\n",
    "        \n",
    "        x = self.linear512_numLabel(x)  \n",
    "           \n",
    "        return x             #[BatchSize, seq_length, numLabel]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End (Defining Classes & Functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# describing and initializing the random seed (the random seed is used to get the same random number in each \n",
    "# repetition of codes)\n",
    "\n",
    "seed = 3054\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "np.random.seed(seed)  # Numpy module.\n",
    "random.seed(seed)  # Python random module.\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# select CPU or GPU to run the program. \n",
    "# n_gpu: 0 used \"CPU\" & n_gpu: 1 used \"GPU\"\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.device(\"cuda\") \n",
    "    n_gpu = torch.cuda.device_count()\n",
    "    print(\"torch.device : cuda\")\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "else:    \n",
    "    torch.device(\"cpu\")\n",
    "    n_gpu = 0;\n",
    "    print(\"torch.device : cpu\")\n",
    "print(n_gpu)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# reading and loading the dataset6\n",
    "\n",
    "data = pd.read_csv(\"./Data/Dokhtara/Dokhtara_Word_Dataset_Main.csv\", encoding=\"utf-8\").fillna(method=\"ffill\")\n",
    "getterTexts = TextGetter('Post #',data).texts\n",
    "\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting the percentage of splitting the dataset\n",
    "\n",
    "valid_size_real = 0.15\n",
    "test_size = 0.30\n",
    "valid_size = valid_size_real /(1- test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffling and splitting the train, valid and test dataset\n",
    "\n",
    "valid_size_real = 0.15\n",
    "test_size = 0.30\n",
    "valid_size = valid_size_real /(1- test_size)\n",
    "Random_State = 2018\n",
    "\n",
    "print('number text:',len(getterTexts),'\\n')\n",
    "\n",
    "trainValid_getter, test_getter = train_test_split(getterTexts, random_state=Random_State, test_size=test_size)\n",
    "train_getter, valid_getter = train_test_split(trainValid_getter,random_state=Random_State, test_size=valid_size)\n",
    "\n",
    "print('number train       :',len(train_getter))\n",
    "print('number valid       :',len(valid_getter))\n",
    "print('number test        :',len(test_getter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# initializing some hyperparameters\n",
    "\n",
    "seq_length = 120\n",
    "BatchSize = 16\n",
    "LRate = 0.0001   # LRate =3e-5     0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assigning a unique index to each label\n",
    "\n",
    "tags_name = list(set(g[1] for gett in getterTexts for g in gett))\n",
    "tags_name.sort()\n",
    "tag2idx = {t: i for i, t in enumerate(tags_name)}\n",
    "num_labels = len(tags_name)\n",
    "\n",
    "TagLabel =tags_name.copy()\n",
    "TagLabel.remove('O')   # without \"O\"\n",
    "\n",
    "print(\"num_labels :\",num_labels)\n",
    "print(\"\")\n",
    "print(\"Tags name  :\",tags_name)\n",
    "print(\"\")\n",
    "print(\"tag2idx    :\",tag2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# a brief report of the distribution of the label in the dataset \n",
    "\n",
    "BNextI = True\n",
    "AllTags = [g[1] for gett in getterTexts for g in gett]\n",
    "report_dataSet = dataset_summary(AllTags, B_NextI= BNextI)\n",
    "\n",
    "TrTags = [g[1] for gett in train_getter for g in gett]\n",
    "report_Train = dataset_summary(TrTags, B_NextI= BNextI)\n",
    "\n",
    "VaTags = [g[1] for gett in valid_getter for g in gett]\n",
    "report_Valid = dataset_summary(VaTags, B_NextI= BNextI)\n",
    "\n",
    "TsTags = [g[1] for gett in test_getter for g in gett]\n",
    "report_Test = dataset_summary(TsTags, B_NextI= BNextI)\n",
    "\n",
    "res = pd.DataFrame({'Label':tags_name})\n",
    "res['Train'] = 0\n",
    "res['Valid'] = 0\n",
    "res['Test'] = 0\n",
    "res['Sum'] = [0]*len(res)\n",
    "for ii in range(len(res)):\n",
    "    \n",
    "    for jj in range(len(report_Train)):\n",
    "        if res['Label'][ii] == report_Train['Label'][jj]:\n",
    "            res['Train'][ii]  = report_Train['Sum'][jj]\n",
    "            break           \n",
    "    \n",
    "    for jj in range(len(report_Valid)):\n",
    "        if res['Label'][ii] == report_Valid['Label'][jj]:\n",
    "            res['Valid'][ii]  = report_Valid['Sum'][jj]\n",
    "            break   \n",
    "            \n",
    "    if len(test_getter) != 0.0 :\n",
    "        for jj in range(len(report_Test)):\n",
    "            if res['Label'][ii] == report_Test['Label'][jj]:\n",
    "                res['Test'][ii]  = report_Test['Sum'][jj]\n",
    "                break\n",
    "            \n",
    "    for jj in range(len(report_dataSet)):\n",
    "        if res['Label'][ii] == report_dataSet['Label'][jj]:\n",
    "            res['Sum'][ii]  = report_dataSet['Sum'][jj]\n",
    "            break\n",
    "            \n",
    "print('*** Data Set ***\\n\\n')\n",
    "print('train (num text)  :',len(train_getter))\n",
    "print('valid (num text)  :',len(valid_getter))\n",
    "print('test  (num text)  :',len(test_getter))\n",
    "print('')\n",
    "print(res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assigning a unique index to each label\n",
    "\n",
    "weights = 'bert-base-uncased'\n",
    "model_B = BertModel.from_pretrained(weights,output_hidden_states = True,num_labels = num_labels)\n",
    "tokenizer = BertTokenizer.from_pretrained(weights, do_lower_case=True, sep_token='[SEP]'\n",
    "                                                 , pad_token='[PAD]', cls_token='[CLS]', mask_token='[MASK]')#PAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# tokenizing and assigning unique indices to tokenized reviews and their related true labels\n",
    "\n",
    "\n",
    "train_tokenized,train_labels = tokenize_and_preserve_labels(train_getter) \n",
    "train_ids,tr_tags,train_masks = ids_and_mask(train_tokenized,train_labels)\n",
    "\n",
    "valid_tokenized,valid_labels = tokenize_and_preserve_labels(valid_getter)\n",
    "valid_ids,val_tags,valid_masks = ids_and_mask(valid_tokenized,valid_labels)\n",
    "\n",
    "test_tokenized,test_labels = tokenize_and_preserve_labels(test_getter) \n",
    "test_ids,ts_tags,test_masks = ids_and_mask(test_tokenized,test_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# a brief report of the distribution of the label in the dataset after BERT tokenization \n",
    "\n",
    "print('### After Tokenized ###')\n",
    "BNextI = True\n",
    "\n",
    "TrTags = [tags_name[l_i] for i,l in enumerate(tr_tags) for ii,l_i in enumerate(l) if train_ids[i][ii] != 0]\n",
    "report_Train = dataset_summary(TrTags , B_NextI= BNextI)\n",
    "\n",
    "VaTags = [tags_name[l_i] for i,l in enumerate(val_tags) for ii,l_i in enumerate(l) if valid_ids[i][ii] != 0]\n",
    "report_Valid = dataset_summary(VaTags , B_NextI= BNextI)\n",
    "\n",
    "TsTags = [tags_name[l_i] for i,l in enumerate(ts_tags) for ii,l_i in enumerate(l) if test_ids[i][ii] != 0]   \n",
    "report_Test = dataset_summary(TsTags , B_NextI= BNextI)\n",
    "\n",
    "res = pd.DataFrame({'Label':TagLabel})\n",
    "res['Train'] = 0\n",
    "res['Valid'] = 0\n",
    "res['Test'] = 0\n",
    "res['Sum'] = [0] *len(res)\n",
    "for ii in range(len(res)):\n",
    "    \n",
    "    for jj in range(len(report_Train)):\n",
    "        if res['Label'][ii] == report_Train['Label'][jj]:\n",
    "            res['Train'][ii]  = report_Train['Sum'][jj]\n",
    "            break           \n",
    "    \n",
    "    for jj in range(len(report_Valid)):\n",
    "        if res['Label'][ii] == report_Valid['Label'][jj]:\n",
    "            res['Valid'][ii]  = report_Valid['Sum'][jj]\n",
    "            break   \n",
    "            \n",
    "    if len(test_getter) != 0.0 :\n",
    "        for jj in range(len(report_Test)):\n",
    "            if res['Label'][ii] == report_Test['Label'][jj]:\n",
    "                res['Test'][ii]  = report_Test['Sum'][jj]\n",
    "                break\n",
    "            \n",
    "    res['Sum'][ii]  = res['Train'][ii]+res['Valid'][ii]+res['Test'][ii]\n",
    "            \n",
    "\n",
    "print('*** Data Set ***\\n')\n",
    "print(res)\n",
    "#Make_Confusion_Graph_Tabel(res,Font_Scale = 1.4,ShowPercent= False, Sum_O_Zero= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create loder objects for train dataset , validation dataset & test dataset\n",
    "\n",
    "#------------- trainloader------------------------\n",
    "x_tr = torch.tensor(train_ids, dtype=torch.long)\n",
    "y_tr = torch.tensor(tr_tags, dtype=torch.float32)\n",
    "m_tr = torch.tensor(train_masks, dtype=torch.int32)\n",
    "\n",
    "train_data = TensorDataset(x_tr, y_tr, m_tr)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "trainloader = DataLoader(train_data, sampler=train_sampler, batch_size=BatchSize)\n",
    "\n",
    "#------------- validloader------------------------\n",
    "x_val = torch.tensor(valid_ids, dtype=torch.long)\n",
    "y_val = torch.tensor(val_tags, dtype=torch.float32)\n",
    "m_val = torch.tensor(valid_masks, dtype=torch.int32)\n",
    "\n",
    "valid_data = TensorDataset(x_val, y_val, m_val)\n",
    "valid_sampler = SequentialSampler(valid_data)\n",
    "validloader = DataLoader(valid_data, sampler=valid_sampler, batch_size=BatchSize)\n",
    "\n",
    "\n",
    "#------------- testloader------------------------\n",
    "x_test = torch.tensor(test_ids, dtype=torch.long)\n",
    "y_test = torch.tensor(ts_tags, dtype=torch.float32)\n",
    "m_test = torch.tensor(test_masks, dtype=torch.int32)\n",
    "\n",
    "test_data = TensorDataset(x_test, y_test, m_test)\n",
    "testloader = DataLoader(test_data, batch_size=BatchSize)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the defined model and check to load in CPU or GPU \n",
    "\n",
    "model = Model()\n",
    "\n",
    "if n_gpu == 1:\n",
    "    model.cuda()\n",
    "else:     \n",
    "    model.cpu()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the loss function\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss(reduction='elementwise_mean',size_average=True, ignore_index=-100)  # LogSoftmax + ClassNLL Loss\n",
    "\n",
    "if n_gpu == 1:\n",
    "    loss_function = loss_function.cuda()\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the optimizer function and setting its parameters\n",
    "\n",
    "FULL_FINETUNING = True\n",
    "if FULL_FINETUNING:\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = ['bias', 'gamma', 'beta']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "         'weight_decay_rate': 0.01},\n",
    "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "         'weight_decay_rate': 0.0}\n",
    "    ]\n",
    "else:\n",
    "    param_optimizer = list(model.classifier.named_parameters())\n",
    "    optimizer_grouped_parameters = [{\"params\": [p for n, p in param_optimizer]}]\n",
    "\n",
    "optimizer = AdamW(\n",
    "                  optimizer_grouped_parameters,\n",
    "                  lr=LRate,\n",
    "                  eps=1e-8\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##### Training  process#####\n",
    "\n",
    "startA = datetime.now()\n",
    "print(\"Now Time : \",startA)\n",
    "\n",
    "\n",
    "show_confusion_after_any_epoch = True\n",
    "show_fclassification_report_after_any_epoch = True\n",
    "Digits = 4\n",
    "\n",
    "validat_loss = 0\n",
    "min_valid_loss = 1000\n",
    "df_score = pd.DataFrame()\n",
    "\n",
    "max_valid_accuracy = 0\n",
    "max_valid_F1Score = 0\n",
    "\n",
    "yLow_loss = 0.001   # for Change\n",
    "yHigh_loss = 2.0    # for Change\n",
    "yLow_accuracy = 0  # for Change\n",
    "yLow_f1_score = 0.0  # for Change\n",
    "\n",
    "Tr_f1Score,Tr_RecallScore,Tr_PreciScore,Tr_Accure =[],[],[],[]\n",
    "Va_f1Score,Va_RecallScore,Va_PreciScore,Va_Accure =[],[],[],[]\n",
    "\n",
    "min_valid_loss_epoch = 0\n",
    "max_valid_F1Score_epoch = 0\n",
    "\n",
    "epoch = 1\n",
    "Average = 'micro'   \n",
    "mode_score = 'strict' \n",
    "Flag_Traning_Countinue = True\n",
    "Epochs_Max = 60\n",
    "#for epoch in range(1, Epochs_Max+1):\n",
    "while Flag_Traning_Countinue == True:\n",
    "    start = datetime.now()\n",
    "    print (\"Epoch :\", epoch,\"                             \")\n",
    "    \n",
    "    #------------ train part ------------------------\n",
    "    train_loss, valid_loss = [], []\n",
    "    predictions = []\n",
    "    true_labels = []  \n",
    "    data2 =[]\n",
    "    ii = 0\n",
    "    model.train()\n",
    "    for data,target, mask in trainloader:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        target = torch.tensor(target, dtype=torch.long)\n",
    "        \n",
    "        if n_gpu == 1:\n",
    "            data = data.to('cuda')\n",
    "            target = target.to('cuda')\n",
    "            mask = mask.to('cuda')  \n",
    "            \n",
    "        output= model(data,mask)\n",
    "        loss = loss_function(output.flatten(start_dim=0, end_dim=1),target.flatten())\n",
    "        \n",
    "        ##  backward propagation\n",
    "        loss.backward()\n",
    "\n",
    "        ##  weight optimization\n",
    "        torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss.append(loss.item())\n",
    "        \n",
    "        label_ids = target.to('cpu').numpy()\n",
    "        true_labels.extend(label_ids)\n",
    "        output2 = output.detach().cpu().numpy()\n",
    "        \n",
    "        data_tmp = data.to('cpu').numpy()\n",
    "        data2.extend(data_tmp)\n",
    "\n",
    "        predictions.extend([list(p) for p in np.argmax(output2, axis=2)]) \n",
    "        ii +=1\n",
    "        print(\"train batch:  %.f / %.f \" %(ii,len(train_ids)/BatchSize),end='\\r')\n",
    "    print(\"                                \",end='\\r')\n",
    "    training_loss = np.mean(train_loss)\n",
    "    \n",
    "    train_pred_tags = [[tags_name[l_i] for ii,l_i in enumerate(l)  if data2[i][ii] != 0 and data2[i][ii] != 101 and data2[i][ii] != 102] for i,l in enumerate(predictions)]\n",
    "    train_tags = [[tags_name[l_i] for ii,l_i in enumerate(l)  if data2[i][ii] != 0 and data2[i][ii] != 101 and data2[i][ii] != 102] for i,l in enumerate(true_labels)]\n",
    "    \n",
    "    Train_Acc = accuracy_score(train_tags, train_pred_tags)*100.0\n",
    "    train_F1Score = f1_score(train_tags, train_pred_tags,average=Average,scheme=IOB2, mode= mode_score )\n",
    "    train_precision_score = precision_score(train_tags, train_pred_tags,average=Average,scheme=IOB2, mode= mode_score)\n",
    "    train_recall_score = recall_score(train_tags, train_pred_tags,average=Average,scheme=IOB2, mode= mode_score)\n",
    "\n",
    "    ## --------------------evaluation part -------------------------\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "    ii = 0\n",
    "    data2 =[]\n",
    "    for data, target, mask in validloader:\n",
    "        target = torch.tensor(target, dtype=torch.long)\n",
    "        if n_gpu == 1:\n",
    "            data = data.to('cuda')\n",
    "            target = target.to('cuda')\n",
    "            mask = mask.to('cuda')\n",
    "            \n",
    "        with torch.no_grad():\n",
    "            output= model(data,mask)\n",
    "\n",
    "        loss = loss_function(output.flatten(start_dim=0, end_dim=1),target.flatten())\n",
    "        \n",
    "        \n",
    "        valid_loss.append(loss.item())\n",
    "        \n",
    "        label_ids = target.to('cpu').numpy()\n",
    "        true_labels.extend(label_ids)\n",
    "        output2 = output.detach().cpu().numpy()\n",
    "        \n",
    "        data_tmp = data.to('cpu').numpy()\n",
    "        data2.extend(data_tmp)\n",
    "        \n",
    "        predictions.extend([list(p) for p in np.argmax(output2, axis=2)]) \n",
    "        ii += 1\n",
    "        print(\"valid batch:  %.f / %.f \" %(ii,len(valid_ids)/BatchSize),end='\\r')\n",
    "\n",
    "\n",
    "    validat_loss = np.mean(valid_loss) \n",
    "    \n",
    "    valid_pred_tags = [[tags_name[l_i] for ii,l_i in enumerate(l)  if data2[i][ii] != 0 and data2[i][ii] != 101 and data2[i][ii] != 102] for i,l in enumerate(predictions)]\n",
    "    valid_tags = [[tags_name[l_i] for ii,l_i in enumerate(l)  if data2[i][ii] != 0 and data2[i][ii] != 101 and data2[i][ii] != 102] for i,l in enumerate(true_labels)]\n",
    "\n",
    "    \n",
    "    Valid_Acc = accuracy_score(valid_tags,valid_pred_tags)*100.0\n",
    "    valid_F1Score = f1_score(valid_tags,valid_pred_tags,average=Average,scheme=IOB2, mode= mode_score )\n",
    "    valid_precision_score = precision_score(valid_tags,valid_pred_tags,average=Average,scheme=IOB2, mode= mode_score)\n",
    "    valid_recall_score = recall_score(valid_tags,valid_pred_tags,average=Average,scheme=IOB2, mode= mode_score)\n",
    "    \n",
    "    #----------- Svae model in min Loss Validation------------------\n",
    "    \n",
    "    if validat_loss < min_valid_loss :\n",
    "        torch.save(model,'./model/Valid_Min_Loss_Model.pt')\n",
    "        min_valid_loss = validat_loss\n",
    "        min_valid_loss_epoch = epoch\n",
    "    \n",
    "    # ------------------- Report & Graph -------------------------\n",
    "    if epoch == 1:\n",
    "        temp = pd.DataFrame({\n",
    "                             'Tr_accur':[0],\n",
    "                             'Va_accur':[0],\n",
    "                             'Tr_loss' : [0],\n",
    "                             'Va_loss' : [0],\n",
    "                             'Tr_F1':[0],\n",
    "                             'Va_F1':[0],\n",
    "                             'Tr_precis':[0],  \n",
    "                             'Va_precis':[0],\n",
    "                             'Tr_recall':[0],  \n",
    "                             'Va_recall':[0]\n",
    "                             }) \n",
    "        df_score = df_score.append(temp, ignore_index = True) \n",
    "    temp = pd.DataFrame({\n",
    "                             'Tr_accur':[Train_Acc],\n",
    "                             'Va_accur':[Valid_Acc],\n",
    "                             'Tr_loss' : [training_loss],\n",
    "                             'Va_loss' : [validat_loss],\n",
    "                             'Tr_F1':[train_F1Score],  \n",
    "                             'Va_F1':[valid_F1Score],\n",
    "                             'Tr_precis':[train_precision_score],  \n",
    "                             'Va_precis':[valid_precision_score],\n",
    "                             'Tr_recall':[train_recall_score],  \n",
    "                             'Va_recall':[valid_recall_score]\n",
    "                             })    \n",
    "    df_score = df_score.append(temp, ignore_index = True)\n",
    "     \n",
    "    print (Fore.BLUE +\"            Accuracy |     Loss    |   Precision |    Recall   |   F1-score  |\"+Fore.RESET)\n",
    "    print('Training  :  %.2f %% |    %.4f   |    %.4f   |    %.4f   |    %.4f   |'\n",
    "           %(Train_Acc,training_loss,train_precision_score,train_recall_score,train_F1Score))\n",
    "    print('Validation:  %.2f %% |    %.4f   |    %.4f   |    %.4f   |    %.4f   |'\n",
    "           %(Valid_Acc,validat_loss,valid_precision_score,valid_recall_score,valid_F1Score))\n",
    "\n",
    "\n",
    "    plot_loss_accuracy(df_score,0, Epochs_Max, yLow_loss,yHigh_loss, yLow_accuracy, 1)\n",
    "    plot_F1_Score(df_score,0, Epochs_Max, yLow_f1_score, 1)\n",
    "      \n",
    "    if show_confusion_after_any_epoch :\n",
    "        print('\\nTraining------------------------------------------------------------------------------------')\n",
    "        Make_Confusion_Graph_Tabel(sum(train_tags,[]), sum(train_pred_tags,[]), Labels= TagLabel, Font_Scale = 1.4,\n",
    "                                                                    Prefix = True, ShowPercent= True, Sum_O_Zero= True)\n",
    "        print('\\n\\nValidation----------------------------------------------------------------------------------')\n",
    "        Make_Confusion_Graph_Tabel(sum(valid_tags,[]), sum(valid_pred_tags,[]), Labels= TagLabel, Font_Scale = 1.4,\n",
    "                                                                    Prefix = True, ShowPercent= True, Sum_O_Zero= True)\n",
    "        print('--------------------------------------------------------------------------------------------')\n",
    "        \n",
    "    if show_fclassification_report_after_any_epoch:\n",
    "        print('\\nTraining------------------------------------------------------------------------------------')\n",
    "        print(classification_report(train_tags, train_pred_tags, digits = Digits,scheme=IOB2, mode= mode_score))\n",
    "\n",
    "        print('\\n\\nValidation----------------------------------------------------------------------------------')\n",
    "        print(classification_report(valid_tags,valid_pred_tags, digits = Digits,scheme=IOB2, mode= mode_score))\n",
    "        print('-----------------------------------------------------------------------------------------------')\n",
    "       \n",
    "    end = datetime.now()\n",
    "    print(\"\")\n",
    "    print(\"Time : \", (end - start))\n",
    "    \n",
    "    print(\"________________________________________________________________________________________________________\")\n",
    "    \n",
    "#------------------------- Trainning Countine Condition Check ---------------------\n",
    "    if epoch - min_valid_loss_epoch > 10 :\n",
    "        Flag_Traning_Countinue = False\n",
    "        Epochs_Max = epoch\n",
    "    else:    \n",
    "        epoch += 1             \n",
    "#----------------------------------------------------------------------------------        \n",
    "plot_loss_accuracy(df_score,0, Epochs_Max, yLow_loss,yHigh_loss, yLow_accuracy, 1)\n",
    "plot_F1_Score(df_score,0, Epochs_Max, yLow_f1_score, 1)        \n",
    "\n",
    "\n",
    "print(\"Sum Times : \", (end - startA))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Nodel (with Test Dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# defining the test function using the model that saved in minimum validation loss and \n",
    "# applying it to the test dataset for prediction\n",
    "\n",
    "def select_model(model):\n",
    "    if n_gpu == 1:\n",
    "        model.cuda()\n",
    "    else:     \n",
    "        model.cpu()\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "    data2 =[]\n",
    "    true_labels_rep = []\n",
    "    test_loss = []\n",
    "    jj = 0\n",
    "    allPred =[]\n",
    "    outputs_test = []\n",
    "    \n",
    "    model.eval()\n",
    "    for data, target,mask in testloader:\n",
    "    \n",
    "        target = torch.tensor(target, dtype=torch.long)\n",
    "        if n_gpu == 1:\n",
    "            data = data.to('cuda')\n",
    "            target = target.to('cuda')\n",
    "            mask = mask.to('cuda')  \n",
    "        with torch.no_grad():\n",
    "            output= model(data,mask)\n",
    "        loss = loss_function(output.flatten(start_dim=0, end_dim=1),target.flatten())\n",
    "        \n",
    "        test_loss.append(loss.item())\n",
    "    \n",
    "\n",
    "        label_ids = target.to('cpu').numpy()\n",
    "\n",
    "        true_labels.extend(label_ids)\n",
    "        true_labels_rep.append(label_ids)\n",
    "        \n",
    "        output2 = output.detach().cpu().numpy()\n",
    "        \n",
    "        data_tmp = data.to('cpu').numpy()\n",
    "        data2.extend(data_tmp)\n",
    "        \n",
    "        predictions.extend([list(p) for p in np.argmax(output2, axis=2)])\n",
    "        \n",
    "        outputs_test.extend(output.detach().cpu().tolist())\n",
    "    \n",
    "        jj += 1\n",
    "        print(\"Test batch:  %.f / %.f \" %(jj,len(x_test)/BatchSize),end='\\r')\n",
    "        \n",
    "        \n",
    "    outputs_test = torch.tensor(outputs_test, dtype=torch.float32).sigmoid()\n",
    "    outputs_test = np.array(outputs_test)    \n",
    "  \n",
    "    Test_Loss= np.mean(test_loss)\n",
    "\n",
    "    test_pred_tags = [[tags_name[l_i] for ii,l_i in enumerate(l)  if data2[i][ii] != 0 and data2[i][ii] != 101 and data2[i][ii] != 102] for i,l in enumerate(predictions)]\n",
    "    test_tags = [[tags_name[l_i] for ii,l_i in enumerate(l)  if data2[i][ii] != 0 and data2[i][ii] != 101 and data2[i][ii] != 102] for i,l in enumerate(true_labels)]\n",
    "\n",
    "    Test_Acc = accuracy_score(test_tags,test_pred_tags)*100.0\n",
    "    test_F1Score = f1_score(test_tags,test_pred_tags,average=Average,scheme=IOB2, mode= mode_score )\n",
    "    test_precision_score = precision_score(test_tags,test_pred_tags,average=Average,scheme=IOB2, mode= mode_score)\n",
    "    test_recall_score = recall_score(test_tags,test_pred_tags,average=Average,scheme=IOB2, mode= mode_score)\n",
    "\n",
    "    print (Fore.BLUE +\"       Accuracy |     Loss    |   Precision |    Recall   |   F1-score  |\"+Fore.RESET)\n",
    "    print('Test :  %.2f %% |    %.4f   |    %.4f   |    %.4f   |    %.4f   |'\n",
    "       %(Test_Acc,Test_Loss,test_precision_score,test_recall_score,test_F1Score))\n",
    "    return test_tags,test_pred_tags,outputs_test\n",
    "\n",
    "model.cpu()\n",
    "model=torch.load('./model/Valid_Min_Loss_Model.pt')\n",
    "print('Test Model Parameter: Valid Minimum Loss (epoch=',min_valid_loss_epoch,')\\n')\n",
    "test_tags,test_pred_tags,outputs = select_model(model)\n",
    "print(\"=========================================================================\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# test dataset all reports (classification report, confusion matrix, and some predicted reviews sample)\n",
    "\n",
    "print('\\n*** Test Model Parameter: Valid Minimum Loss (epoch=',min_valid_loss_epoch, ') ***\\n')\n",
    "\n",
    "prediction = test_pred_tags.copy()\n",
    "output = outputs.copy()\n",
    "    \n",
    "OutsAll = []\n",
    "for ii in range(len(test_tags)):\n",
    "    OutsLine = []\n",
    "    for jj in range(len(test_tags[ii])):\n",
    "        OutsLine.append(output[ii,jj+1,:])\n",
    "    OutsAll.extend([OutsLine])    \n",
    "    \n",
    "#group_sel = group_name.replace('#',':')\n",
    "TestId, TestPo, TestSe = [], [], []\n",
    "TestWords, TestTags, TestPreds, TestChecks = [], [], [], []\n",
    "\n",
    "for sent in test_getter: \n",
    "    Id, Po, Se = [], [], []\n",
    "    Words, Tags, Preds, Checks = [], [], [], []\n",
    "    for ii,s in enumerate(sent):\n",
    "        Words.extend([s[0]])        \n",
    "        Tags.extend([s[1]]) \n",
    "        Preds.extend([''])\n",
    "        Checks.extend([''])\n",
    "        if ii == 0 :\n",
    "            Id.extend([s[2]])\n",
    "            Po.extend([s[3]])\n",
    "            Se.extend([s[4]])\n",
    "        else:\n",
    "            Id.extend([''])\n",
    "            Po.extend([''])\n",
    "            Se.extend([''])\n",
    "        \n",
    "    TestWords.extend([Words])\n",
    "    TestTags.extend([Tags])\n",
    "    TestId.extend([Id])\n",
    "    TestPo.extend([Po])\n",
    "    TestSe.extend([Se])\n",
    "    TestPreds.extend([Preds])\n",
    "    TestChecks.extend([Checks]) \n",
    "    \n",
    "\n",
    "for ii in range(len(TestWords)):\n",
    "\n",
    "    kk = 0   \n",
    "    \n",
    "    for jj in range(len(TestWords[ii])):\n",
    "        try:\n",
    "            TestPreds[ii][jj] = prediction[ii][kk]\n",
    "            kk += len(tokenizer.tokenize(TestWords[ii][jj])) \n",
    "            if TestTags[ii][jj] == TestPreds[ii][jj]:\n",
    "                TestChecks[ii][jj] = 'True'  \n",
    "            else:\n",
    "                TestChecks[ii][jj] = 'False'    \n",
    "        except:\n",
    "            break\n",
    "for ii in range(len(TestPreds)-1,0,-1) :\n",
    "    for jj in range(len(TestPreds[ii])-1,0,-1):\n",
    "        if TestPreds[ii][jj] == '':\n",
    "            del TestId[ii][jj]\n",
    "            del TestPo[ii][jj]\n",
    "            del TestSe[ii][jj]\n",
    "            del TestWords[ii][jj]\n",
    "            del TestTags[ii][jj]\n",
    "            del TestPreds[ii][jj]\n",
    "            del TestChecks[ii][jj]\n",
    "            \n",
    " #Svae Test Named Entities Result --------------------------------------------------------\n",
    "df_predict= pd.DataFrame({'Id':sum(TestId,[]),'Post #':sum(TestPo,[]), 'Sentence #':sum(TestSe,[]),\n",
    "                          'Word':sum(TestWords,[]),\n",
    "                          'Tag':sum(TestTags,[]),'Predict': sum(TestPreds,[]),'Check':sum(TestChecks,[])})\n",
    "\n",
    "df_predict.to_csv(\"./Result/Test_Dataset_Name_Entities.csv\",sep=\",\", index=None)\n",
    "#-----------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "Test_Acc = accuracy_score(TestTags,TestPreds)*100.0\n",
    "test_F1Score = f1_score(TestTags,TestPreds,average=Average,scheme=IOB2, mode= mode_score )\n",
    "test_precision_score = precision_score(TestTags,TestPreds,average=Average,scheme=IOB2, mode= mode_score)\n",
    "test_recall_score = recall_score(TestTags,TestPreds,average=Average,scheme=IOB2, mode= mode_score)\n",
    "\n",
    "print ('===========================================================')\n",
    "print (Fore.BLUE +\"       Accuracy |  Precision  |    Recall   |   F1-score  |\"+Fore.RESET)\n",
    "print('Test :  %.2f %% |    %.4f   |    %.4f   |    %.4f   |'\n",
    "   %(Test_Acc,test_precision_score,test_recall_score,test_F1Score))\n",
    "print ('===========================================================')\n",
    "\n",
    "#Prints-----------------------------------------------------------------------------------\n",
    "from seqeval.metrics import  classification_report\n",
    "print('----------------------------------------------------------------------------')\n",
    "print('mode: strict')\n",
    "result_report = classification_report(TestTags,TestPreds, digits = Digits,scheme=IOB2, mode= 'strict')\n",
    "print(result_report)\n",
    "print('----------------------------------------------------------------------------')\n",
    "print('library: sklearn')\n",
    "from sklearn.metrics import  classification_report\n",
    "result_report = classification_report(sum(TestTags,[]),sum(TestPreds,[]), digits = Digits,labels =TagLabel)\n",
    "print(result_report)\n",
    "from seqeval.metrics import  classification_report\n",
    "\n",
    "print('---------------------------------------------------------------------------\\n')\n",
    "\n",
    "Make_Confusion_Graph_Tabel(sum(TestTags,[]),sum(TestPreds,[]), Labels= tags_name, Font_Scale = 1.4,\n",
    "                                                                    Prefix = True, ShowPercent= True, Sum_O_Zero= True)\n",
    "Make_Confusion_Graph_Tabel(sum(TestTags,[]),sum(TestPreds,[]), Labels= tags_name, Font_Scale = 1.4,\n",
    "                                                                    Prefix = True, ShowPercent= False, Sum_O_Zero= True)\n",
    "df_predict.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BERT4",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
